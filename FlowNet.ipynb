{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FlowNet.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMt1tadmSGtxJYi0FQscPYy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeremy26/video_analysis_course/blob/main/FlowNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVdTJsytfFL9"
      },
      "source": [
        "# FlowNet\n",
        "\n",
        "In this project, we're going to build a FlowNet algorithm with PyTorch! The idea is simple, given two images, output the optical flow!\n",
        "<p>\n",
        "\n",
        "![flownet](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ_p_REZwjQ1YqfV51j8vQ1qJodRUDRI8Dd7tPuwbWW-tWUQBhKibGi3Bq1ox6SNp5k2ts&usqp=CAU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UvxRbhEhgDl"
      },
      "source": [
        "Let's begin with some synchronization and imports!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIyNEHieihui"
      },
      "source": [
        "!wget https://thinkautonomous-flownet.s3.eu-west-3.amazonaws.com/flownet-data.zip && unzip flownet-data.zip && rm flownet-data.zip\n",
        "!mkdir output\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZFfHIHefoc6"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_ynPeGbfXoO"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from google.colab.patches import cv2_imshow\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk6ylzzCNnPs"
      },
      "source": [
        "from __future__ import division\n",
        "import os.path\n",
        "import torch.utils.data as data\n",
        "import os\n",
        "from imageio import imread\n",
        "import numbers\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import shutil\n",
        "import torch.nn as nn\n",
        "from torch.nn.init import kaiming_normal_, constant_\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9Wav9zINSlK"
      },
      "source": [
        "## Data\n",
        "\n",
        "There are a few Optical Flow Datasets we can use:\n",
        "\n",
        "*   Flying Chairs\n",
        "*   Scene Flow (KITTI)\n",
        "*   Middleburry\n",
        "*   MPI Sintel\n",
        "*   Kinetics\n",
        "\n",
        "For the purpose of this course, we'll use KITTI as it's the closest to autonomous driving. But if you want to use another one, you do you and I do me. Just a note, you'll need to read the flownet paper as some architectures and techniques work best on some datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS0JmLEIzV6J"
      },
      "source": [
        "images_dataset = sorted(glob.glob(\"dataset/images_2/*.png\"))\n",
        "labels_dataset = sorted(glob.glob(\"dataset/flow_occ/*.png\"))\n",
        "\n",
        "print(len(images_dataset))\n",
        "print(len(labels_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVTt3HHJ8VZS"
      },
      "source": [
        "### 1) Visualize Data & Build a Dataset\n",
        "\n",
        "Here's what we want:\n",
        "*   **Input:** A pair of images (t0 and t1)\n",
        "*   **Labels:** The Flow Map\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsYolQyRLCBV"
      },
      "source": [
        "def bgr2rgb(image):\n",
        "    \"\"\"\n",
        "    Convert BGR TO RGB\n",
        "    \"\"\"\n",
        "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd6VvYSO2Sm9"
      },
      "source": [
        "\"\"\"\n",
        "Make a Dataset\n",
        "\"\"\"\n",
        "\n",
        "images = []\n",
        "for flow_map in labels_dataset:\n",
        "    root_filename = flow_map[-13:-7]\n",
        "    img1 = os.path.join(\"dataset/images_2/\", root_filename+'_10.png')\n",
        "    img2 = os.path.join(\"dataset/images_2/\", root_filename+'_11.png')\n",
        "    images.append([[img1, img2], flow_map])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAGFyoiMUXB6"
      },
      "source": [
        "def load_flow_from_png(png_path):\n",
        "    '''\n",
        "    This is used to read flow label images from the KITTI Dataset\n",
        "    '''\n",
        "    flo_file = cv2.imread(png_path, -1) # The Image is a 16 Bit Image. We must read it with OpenCV and the flag cv2.IMREAD_UNCHANGED (-1)\n",
        "    flo_img = flo_file[:,:,2:0:-1].astype(np.float32)\n",
        "\n",
        "    # See the README File in the KITTI DEVKIT AND THE FLOW READER FUNCTIONS\n",
        "    invalid = (flo_file[:,:,0] == 0)\n",
        "    flo_img = flo_img - 32768\n",
        "    flo_img = flo_img / 64\n",
        "\n",
        "    # Valid and Small Flow = 1e-10\n",
        "    flo_img[np.abs(flo_img) < 1e-10] = 1e-10\n",
        "\n",
        "    # Invalid Flow = 0\n",
        "    flo_img[invalid, :] = 0\n",
        "\n",
        "    return flo_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODRSJrlkWHR4"
      },
      "source": [
        "### TEST\n",
        "idx = 100\n",
        "flo_path = images[idx][1]\n",
        "#yuv = load_flow_from_png(flo_path)\n",
        "yuv = cv2.imread(flo_path, -1)\n",
        "#rgb_map = cv2.cvtColor(yuv, cv2.COLOR_YCR_CB2RGB) \n",
        "rgb_map = cv2.cvtColor(yuv, cv2.COLOR_YUV2RGB) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuTsJhIQ_a7j"
      },
      "source": [
        "\"\"\"\n",
        "Visualize the Data\n",
        "\"\"\"\n",
        "idx = 100\n",
        "\n",
        "image_t0 = bgr2rgb(cv2.imread(images[idx][0][0]))\n",
        "image_t1 = bgr2rgb(cv2.imread(images[idx][0][1]))\n",
        "\n",
        "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30,20))\n",
        "ax1.imshow(image_t0)\n",
        "ax1.set_title('Image t0', fontsize=30)\n",
        "ax2.imshow(image_t1)\n",
        "ax2.set_title('Image t1', fontsize=30)\n",
        "ax3.imshow(rgb_map)\n",
        "ax3.set_title(\"Flow (label)\", fontsize=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKFtwbTeAlhv"
      },
      "source": [
        "Now, we're going to do the same thing, but we'll split the images and laels into Training and Testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm4Ael2nKEA5"
      },
      "source": [
        "def train_test_split(images, default_split=0.8):\n",
        "    \"\"\"\n",
        "    Transforms the Dataset into Train/Test\n",
        "    \"\"\"\n",
        "    split_values = np.random.uniform(0,1,len(images)) < default_split # Randomly decides if an image is train or test\n",
        "    train_samples = [sample for sample, split in zip(images, split_values) if split]\n",
        "    test_samples = [sample for sample, split in zip(images, split_values) if not split]\n",
        "    return train_samples, test_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu2jUoJiFUf1"
      },
      "source": [
        "## Send our Loaded Images and Labels into the train/test split\n",
        "train_samples, test_samples = train_test_split(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haF5_QQKwIlV"
      },
      "source": [
        "print(len(train_samples))\n",
        "print(len(test_samples))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZagbPEE98f3"
      },
      "source": [
        "### 2) Load the Images\n",
        "\n",
        "So far, we have two variables; train_samples and test_samples. The problem is that both only contain URLs for our data; and not the actual data (above is just separate visualization).<p>\n",
        "\n",
        "We'll have to create a function to load the images based on the paths, and one to load the optical flow images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGhJaSV_N0Wy"
      },
      "source": [
        "def KITTI_loader(root,path_imgs, path_flo):\n",
        "    \"\"\"\n",
        "    Returns the Loaded Images in RGB, and the Loaded Optical Flow Labels\n",
        "    \"\"\"\n",
        "    imgs = [os.path.join(root,path) for path in path_imgs]\n",
        "    flo = os.path.join(root,path_flo)\n",
        "    return [cv2.imread(img)[:,:,::-1].astype(np.float32) for img in imgs],load_flow_from_png(flo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJUk8giLtV5G"
      },
      "source": [
        "### 3) Build a Dataset Class & Augment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5FzlcKifku4"
      },
      "source": [
        "import torch.utils.data as data\n",
        "import flow_transforms\n",
        "\n",
        "div_flow = 20 #Value by which flow will be divided. Original value is 20 but 1 with batchNorm gives good results.\n",
        "\n",
        "\"\"\"\n",
        "Div Flow is a factor by which we divide the output (thus >=1). It makes training more stable to deal with low numbers than big ones.\n",
        "\"\"\"\n",
        "\n",
        "# Define transforms for the Inputs\n",
        "input_transform = transforms.Compose([flow_transforms.ArrayToTensor(), transforms.Normalize(mean=[0,0,0], std=[255,255,255]), transforms.Normalize(mean=[0.45,0.432,0.411], std=[1,1,1])])\n",
        "\"\"\"\n",
        "In these transforms, we essentially normalize the dataset around the values. In the Flying Chair Dataset, the mean is [0.45,0.432,0.411].\n",
        "https://github.com/ClementPinard/FlowNetPytorch/issues/101#issuecomment-805222823\n",
        "\"\"\"\n",
        "\n",
        "# Define transforms for the Labels\n",
        "target_transform = transforms.Compose([flow_transforms.ArrayToTensor(),transforms.Normalize(mean=[0,0],std=[div_flow,div_flow])])\n",
        "\n",
        "# Data Augmentation\n",
        "co_transform = flow_transforms.Compose([flow_transforms.RandomCrop((320,448)), flow_transforms.RandomVerticalFlip(),flow_transforms.RandomHorizontalFlip()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZzU_k7JOnh_"
      },
      "source": [
        "class ListDataset(data.Dataset):\n",
        "    def __init__(self, path_list, transform=None, target_transform=None, co_transform=None, loader=KITTI_loader):\n",
        "        self.root = os.getcwd()\n",
        "        self.path_list = path_list\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.co_transform = co_transform\n",
        "        self.loader = loader\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        In Python, __getitem__ is used to read values from a class. For example; read the transformed input files.\n",
        "        Instead of calling the function .read(), we use __getitem__ to directly get the value.\n",
        "        Similarly, __setitem__ can be used to fill values in a class.\n",
        "        \"\"\"\n",
        "        inputs, target = self.path_list[index]\n",
        "        inputs, target = self.loader(self.root, inputs, target)\n",
        "        if self.co_transform is not None:\n",
        "            inputs, target = self.co_transform(inputs, target)\n",
        "        if self.transform is not None:\n",
        "            inputs[0] = self.transform(inputs[0])\n",
        "            inputs[1] = self.transform(inputs[1])\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "        return inputs, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.path_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpFjzrw4Q-fb"
      },
      "source": [
        "train_dataset = ListDataset(train_samples, input_transform, target_transform, co_transform, loader=KITTI_loader)\n",
        "\n",
        "test_dataset = ListDataset(test_samples, input_transform,target_transform, flow_transforms.CenterCrop((370,1224)), loader=KITTI_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uatdrDg6imYF"
      },
      "source": [
        "ðŸ‘‰ðŸ‘‰ðŸ‘‰ The Output of Module 1 are train_dataset and test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eujz-vKtfp8F"
      },
      "source": [
        "## FlowNet Architecture\n",
        "\n",
        "FlowNet has two variations:\n",
        "*   **FlownetS** or Simple, which is a simple version using 2D Convolutions to get to the optical flow computation\n",
        "*   **FlownetC** or Correlated, which adds a correlation layer and process images separately\n",
        "\n",
        "![](https://www.programmersought.com/images/62/6e44fdd78581acc06614a38d8eaff3fe.JPEG)\n",
        "\n",
        "In both, there are two main parts:\n",
        "*   An **Encoder** Part, learning features\n",
        "*   A **Refinement** Part, playing the decoder and creating the output Flow Mask.\n",
        "\n",
        "It looks like we've got some work! Let's go!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWKD_Q9wk4XV"
      },
      "source": [
        "### FlowNet Simple Architecture\n",
        "\n",
        "![](https://miro.medium.com/max/1400/0*XVygX0wF3enVQJLe.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmFL7wBaX7If"
      },
      "source": [
        "#Define a Convolution with or without batchnorm and LeakyReLU\n",
        "\n",
        "def conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1):\n",
        "    if batchNorm:\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=False),\n",
        "            nn.BatchNorm2d(out_planes),\n",
        "            nn.LeakyReLU(0.1,inplace=True)\n",
        "        )\n",
        "    else:\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=True),\n",
        "            nn.LeakyReLU(0.1,inplace=True)\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFUxwdcJYuoo"
      },
      "source": [
        "Here's the refinement block:<p>\n",
        "![](https://i1.wp.com/syncedreview.com/wp-content/uploads/2017/09/image-14.png?fit=692%2C268&ssl=1)\n",
        "<p>\n",
        "\n",
        "It looks like we're gonna need:\n",
        "\n",
        "*   **Deconvolutions** as an upsampling method.\n",
        "*   and **Flow Prediction** as a way to get the output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MdJOXrKYJh9"
      },
      "source": [
        "#Define the last convolution (optical flow map prediction)\n",
        "def predict_flow(in_planes):\n",
        "    return nn.Conv2d(in_planes,2,kernel_size=3,stride=1,padding=1,bias=False) # Note: In the paper, a kernel Size of 3 is written; but their implementation uses 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mazjo3pYPjv"
      },
      "source": [
        "#Define a Deconvolution\n",
        "def deconv(in_planes, out_planes):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_planes, out_planes, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "        nn.LeakyReLU(0.1,inplace=True)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ1u-2DMfn5j"
      },
      "source": [
        "#Define a Cropping Operation\n",
        "def crop_like(input, target):\n",
        "    if input.size()[2:] == target.size()[2:]:\n",
        "        return input\n",
        "    else:\n",
        "        return input[:, :, :target.size(2), :target.size(3)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VARY7bulbCV"
      },
      "source": [
        "class FlowNetS(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self,batchNorm=True):\n",
        "        super(FlowNetS,self).__init__()\n",
        "\n",
        "        #ENCODER PART\n",
        "        self.batchNorm = batchNorm\n",
        "        self.conv1   = conv(self.batchNorm,   6,   64, kernel_size=7, stride=2)\n",
        "        self.conv2   = conv(self.batchNorm,  64,  128, kernel_size=5, stride=2)\n",
        "        self.conv3   = conv(self.batchNorm, 128,  256, kernel_size=5, stride=2)\n",
        "        self.conv3_1 = conv(self.batchNorm, 256,  256)\n",
        "        self.conv4   = conv(self.batchNorm, 256,  512, stride=2)\n",
        "        self.conv4_1 = conv(self.batchNorm, 512,  512)\n",
        "        self.conv5   = conv(self.batchNorm, 512,  512, stride=2)\n",
        "        self.conv5_1 = conv(self.batchNorm, 512,  512)\n",
        "        self.conv6   = conv(self.batchNorm, 512, 1024, stride=2)\n",
        "        self.conv6_1 = conv(self.batchNorm,1024, 1024) # Note: This one doesn't exist in the paper, but it does in their implementation\n",
        "\n",
        "        #REFINEMENT PART\n",
        "        self.deconv5 = deconv(1024,512)\n",
        "        self.deconv4 = deconv(1026,256)\n",
        "        self.deconv3 = deconv(770,128)\n",
        "        self.deconv2 = deconv(386,64)\n",
        "\n",
        "        self.predict_flow6 = predict_flow(1024)\n",
        "        self.predict_flow5 = predict_flow(1026)\n",
        "        self.predict_flow4 = predict_flow(770)\n",
        "        self.predict_flow3 = predict_flow(386)\n",
        "        self.predict_flow2 = predict_flow(194)\n",
        "\n",
        "        self.upsampled_flow6_to_5 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "        self.upsampled_flow5_to_4 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "        self.upsampled_flow4_to_3 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "        self.upsampled_flow3_to_2 = nn.ConvTranspose2d(2, 2, 4, 2, 1, bias=False)\n",
        "\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                # Initialize the Convolutions with \"He Initialization\" to 0.1 (https://arxiv.org/pdf/1502.01852.pdf)\n",
        "                kaiming_normal_(m.weight, 0.1)\n",
        "                if m.bias is not None:\n",
        "                    # Initialize all bias to 0\n",
        "                    constant_(m.bias, 0)\n",
        "            # Initialize the BatchNorm Convolutions with \"He Initialization\" to 1 (https://arxiv.org/pdf/1502.01852.pdf)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                constant_(m.weight, 1)\n",
        "                constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #ENCODER\n",
        "        out_conv2 = self.conv2(self.conv1(x))\n",
        "        out_conv3 = self.conv3_1(self.conv3(out_conv2))\n",
        "        out_conv4 = self.conv4_1(self.conv4(out_conv3))\n",
        "        out_conv5 = self.conv5_1(self.conv5(out_conv4))\n",
        "        out_conv6 = self.conv6_1(self.conv6(out_conv5))\n",
        "        \n",
        "        #REFINEMENT\n",
        "        flow6       = self.predict_flow6(out_conv6)\n",
        "        flow6_up    = crop_like(self.upsampled_flow6_to_5(flow6), out_conv5)\n",
        "        out_deconv5 = crop_like(self.deconv5(out_conv6), out_conv5)\n",
        "\n",
        "        concat5 = torch.cat((out_conv5,out_deconv5,flow6_up),1)\n",
        "        flow5       = self.predict_flow5(concat5)\n",
        "        flow5_up    = crop_like(self.upsampled_flow5_to_4(flow5), out_conv4)\n",
        "        out_deconv4 = crop_like(self.deconv4(concat5), out_conv4)\n",
        "\n",
        "        concat4 = torch.cat((out_conv4,out_deconv4,flow5_up),1)\n",
        "        flow4       = self.predict_flow4(concat4)\n",
        "        flow4_up    = crop_like(self.upsampled_flow4_to_3(flow4), out_conv3)\n",
        "        out_deconv3 = crop_like(self.deconv3(concat4), out_conv3)\n",
        "\n",
        "        concat3 = torch.cat((out_conv3,out_deconv3,flow4_up),1)\n",
        "        flow3       = self.predict_flow3(concat3)\n",
        "        flow3_up    = crop_like(self.upsampled_flow3_to_2(flow3), out_conv2)\n",
        "        out_deconv2 = crop_like(self.deconv2(concat3), out_conv2)\n",
        "\n",
        "        concat2 = torch.cat((out_conv2,out_deconv2,flow3_up),1)\n",
        "        flow2 = self.predict_flow2(concat2)\n",
        "\n",
        "        if self.training:\n",
        "            return flow2,flow3,flow4,flow5,flow6\n",
        "        else:\n",
        "            return flow2\n",
        "\n",
        "    def weight_parameters(self):\n",
        "        return [param for name, param in self.named_parameters() if 'weight' in name]\n",
        "\n",
        "    def bias_parameters(self):\n",
        "        return [param for name, param in self.named_parameters() if 'bias' in name]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuVGTDSbeagA"
      },
      "source": [
        "Awesome Work! Now, let's define the Flownet S model (with and without batchnorm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "celCeoeVeFYK"
      },
      "source": [
        "#Define FlowNet S\n",
        "def flownets(data=None, batchNorm=False):\n",
        "    \"\"\"FlowNetS model architecture from the\n",
        "    \"Learning Optical Flow with Convolutional Networks\" paper (https://arxiv.org/abs/1504.06852)\n",
        "    Args:\n",
        "        data : pretrained weights of the network. will create a new one if not set\n",
        "    \"\"\"\n",
        "    model = FlowNetS(batchNorm=batchNorm)\n",
        "    if data is not None:\n",
        "        model.load_state_dict(data['state_dict'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWz3NPoFjX21"
      },
      "source": [
        "checkpoint = torch.load(\"models/flownets_bn_EPE2.459.pth.tar\")\n",
        "model = flownets(data=checkpoint, batchNorm=True)\n",
        "\n",
        "#model = flownets()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYVhcBrDjuyg"
      },
      "source": [
        "ðŸ‘‰ðŸ‘‰ðŸ‘‰ The output of Module 2 is this model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pn2GBm7hu86"
      },
      "source": [
        "## Training\n",
        "To train a Deep Learning Model, we'll need:\n",
        "\n",
        "*   Data\n",
        "*   A Model\n",
        "*   Parameters\n",
        "*   A Loss Function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiS3HNkBh_g6"
      },
      "source": [
        "### Data & Model\n",
        "Define the basic variables, paths to save the training weights, dataloaders, and the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtyELv3GD_N0"
      },
      "source": [
        "print('{} samples found, {} train samples and {} test samples '.format(len(test_dataset)+len(train_dataset),\n",
        "                                                                           len(train_dataset),\n",
        "                                                                           len(test_dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq6GuSleh6GR"
      },
      "source": [
        "arch = \"flownetsbn\"\n",
        "solver = \"adam\" # or sgd\n",
        "epochs = 400\n",
        "epoch_size = 0\n",
        "batch_size = 32\n",
        "learning_rate = 10e-4\n",
        "workers = 4\n",
        "pretrained = None\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "save_path = '{},{},{}epochs,b{},lr{}'.format(arch, solver, epochs,batch_size,learning_rate)\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "train_writer = SummaryWriter(os.path.join(save_path,'train'))\n",
        "test_writer = SummaryWriter(os.path.join(save_path,'test'))\n",
        "output_writers = []\n",
        "\n",
        "for i in range(3):\n",
        "    output_writers.append(SummaryWriter(os.path.join(save_path,'test',str(i))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Fdk24_iItq"
      },
      "source": [
        "#With these basic values, create dataloaders (PyTorch way of handling the data)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=workers, pin_memory=True, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,num_workers=workers, pin_memory=True, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0vaSPujkAvD"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6lKNXKMjn7z"
      },
      "source": [
        "bias_decay = 0\n",
        "weight_decay = 4e-4\n",
        "momentum = 0.9 # Momentum for SGD - Alpha for Adam\n",
        "milestones= [100,150,200] # Epochs by which we divide learning rate by 2\n",
        "\n",
        "param_groups = [{'params': model.bias_parameters(), 'weight_decay': bias_decay},\n",
        "                {'params': model.weight_parameters(), 'weight_decay': weight_decay}]\n",
        "\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    model = torch.nn.DataParallel(model).cuda()\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "\n",
        "if solver == 'adam':\n",
        "    optimizer = torch.optim.Adam(param_groups, learning_rate,betas=(momentum, 0.999)) # In the paper, Adam is used\n",
        "elif solver == 'sgd':\n",
        "    optimizer = torch.optim.SGD(param_groups, learning_rate,momentum=momentum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtrOORO5km0P"
      },
      "source": [
        "### Loss Function - End Point Error\n",
        "\n",
        "Flownet (and most optical flow algorithms) use the end point error (EPE) as a metric for the loss function.\n",
        "It is simply the euclidean distance between the real value (ground truth) and the predicted one.<p>\n",
        "EPE = ![](https://latex.codecogs.com/gif.latex?%5Cinline%20%5Cleft%20%5C%7CV_%7Best%7D%20-%20V_%7Bgt%7D%20%5Cright%20%5C%7C)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPj4N7TCuSJ1"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{:.3f} ({:.3f})'.format(self.val, self.avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EtAjWaBtNVt"
      },
      "source": [
        "def EPE(input_flow, target_flow, sparse=False, mean=True):\n",
        "    EPE_map = torch.norm(target_flow-input_flow,2,1)\n",
        "    batch_size = EPE_map.size(0)\n",
        "    if sparse:\n",
        "        # invalid flow is defined with both flow coordinates to be exactly 0\n",
        "        mask = (target_flow[:,0] == 0) & (target_flow[:,1] == 0)\n",
        "        EPE_map = EPE_map[~mask]\n",
        "    if mean:\n",
        "        return EPE_map.mean()\n",
        "    else:\n",
        "        return EPE_map.sum()/batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TGjqGFQpEix"
      },
      "source": [
        "def realEPE(output, target, sparse=False):\n",
        "    b, _, h, w = target.size()\n",
        "    upsampled_output = F.interpolate(output, (h,w), mode='bilinear', align_corners=False) # used to resize the output\n",
        "    return EPE(upsampled_output, target, sparse, mean=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBy4PpK2s8pn"
      },
      "source": [
        "def sparse_max_pool(input, size):\n",
        "    '''Downsample the input by considering 0 values as invalid.\n",
        "    Unfortunately, no generic interpolation mode can resize a sparse map correctly,\n",
        "    the strategy here is to use max pooling for positive values and \"min pooling\"\n",
        "    for negative values, the two results are then summed.\n",
        "    This technique allows sparsity to be minized, contrary to nearest interpolation,\n",
        "    which could potentially lose information for isolated data points.'''\n",
        "\n",
        "    positive = (input > 0).float()\n",
        "    negative = (input < 0).float()\n",
        "    output = F.adaptive_max_pool2d(input * positive, size) - F.adaptive_max_pool2d(-input * negative, size)\n",
        "    return output\n",
        "\n",
        "\n",
        "def multiscaleEPE(network_output, target_flow, weights=None, sparse=False):\n",
        "    def one_scale(output, target, sparse):\n",
        "\n",
        "        b, _, h, w = output.size()\n",
        "        if sparse:\n",
        "            target_scaled = sparse_max_pool(target, (h, w))\n",
        "        else:\n",
        "            target_scaled = F.interpolate(target, (h, w), mode='area')\n",
        "        return EPE(output, target_scaled, sparse, mean=False)\n",
        "    \n",
        "\n",
        "    if type(network_output) not in [tuple, list]:\n",
        "        network_output = [network_output]\n",
        "    if weights is None:\n",
        "        weights = [0.005, 0.01, 0.02, 0.08, 0.32]  # as in original article\n",
        "    assert(len(weights) == len(network_output))\n",
        "\n",
        "    loss = 0\n",
        "    for output, weight in zip(network_output, weights):\n",
        "        loss += weight * one_scale(output, target_flow, sparse)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoKeZlN5Lga6"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNebIhRikoOT"
      },
      "source": [
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o08yr4e5vfV1"
      },
      "source": [
        "def save_checkpoint(state, is_best, save_path, filename='checkpoint.pth.tar'):\n",
        "    torch.save(state, os.path.join(save_path,filename))\n",
        "    if is_best:\n",
        "        shutil.copyfile(os.path.join(save_path,filename), os.path.join(save_path,'model_best.pth.tar'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-OAEj_dO2VO"
      },
      "source": [
        "def train(train_loader, model, optimizer, epoch, train_writer):\n",
        "    global n_iter, div_flow\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    flow2_EPEs = AverageMeter()\n",
        "\n",
        "    multiscale_weights = [0.005,0.01,0.02,0.08,0.32] # from output_flow to flow6\n",
        "\n",
        "    epoch_size = len(train_loader)# if epoch_size == 0 else min(len(train_loader), epoch_size)\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        # Go through the entire data loader\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        target = target.to(device)\n",
        "        input = torch.cat(input,1).to(device)\n",
        "\n",
        "        # Forward Pass\n",
        "        output = model(input)\n",
        "\n",
        "        # Since Target pooling is not very precise when sparse,\n",
        "        # take the highest resolution prediction and upsample it instead of downsampling target\n",
        "        h, w = target.size()[-2:]\n",
        "        output = [F.interpolate(output[0], (h,w)), *output[1:]]\n",
        "\n",
        "        # Compute Multiscale EPE (for all predict flows)\n",
        "        loss = multiscaleEPE(output, target, weights=multiscale_weights, sparse=True)\n",
        "\n",
        "        # Compute the Output EPE\n",
        "        flow2_EPE = div_flow * realEPE(output[0], target, sparse=True)\n",
        "\n",
        "        # Record loss and EPE\n",
        "        losses.update(loss.item(), target.size(0))\n",
        "        train_writer.add_scalar('train_loss', loss.item(), n_iter)\n",
        "        flow2_EPEs.update(flow2_EPE.item(), target.size(0))\n",
        "\n",
        "        # compute gradient and do optimization step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % 2 == 0:\n",
        "            # Every 2 steps, print the Loss and EPE\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t Time {3}\\t Data {4}\\t Loss {5}\\t EPE {6}'\n",
        "                  .format(epoch, i, epoch_size, batch_time,\n",
        "                          data_time, losses, flow2_EPEs))\n",
        "        n_iter += 1\n",
        "        if i >= epoch_size:\n",
        "            break\n",
        "    #Return the Average Loss and Average EPE on the Training Set\n",
        "    return losses.avg, flow2_EPEs.avg\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M29R5t1ATknN"
      },
      "source": [
        "def validate(val_loader, model, epoch, output_writers):\n",
        "    global div_flow\n",
        "    batch_time = AverageMeter()\n",
        "    flow2_EPEs = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(val_loader):\n",
        "        #Go through the entire validation loader\n",
        "\n",
        "        target = target.to(device)\n",
        "        input = torch.cat(input,1).to(device)\n",
        "\n",
        "        # Forward Pass\n",
        "        output = model(input)\n",
        "\n",
        "        #Compute the EPE\n",
        "        flow2_EPE = div_flow*realEPE(output, target, sparse=True)\n",
        "        # record EPE\n",
        "        flow2_EPEs.update(flow2_EPE.item(), target.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i < len(output_writers):  # log first output of first batches\n",
        "            if epoch == 0:\n",
        "                mean_values = torch.tensor([0.45,0.432,0.411], dtype=input.dtype).view(3,1,1)\n",
        "                output_writers[i].add_image('GroundTruth', flow2rgb(div_flow * target[0], max_value=10), 0)\n",
        "                output_writers[i].add_image('Inputs', (input[0,:3].cpu() + mean_values).clamp(0,1), 0)\n",
        "                output_writers[i].add_image('Inputs', (input[0,3:].cpu() + mean_values).clamp(0,1), 1)\n",
        "            output_writers[i].add_image('FlowNet Outputs', flow2rgb(div_flow * output[0], max_value=10), epoch)\n",
        "\n",
        "        if i % 5 == 0:\n",
        "            print('Test: [{0}/{1}]\\t Time {2}\\t EPE {3}'\n",
        "                  .format(i, len(val_loader), batch_time, flow2_EPEs))\n",
        "\n",
        "    print(' * EPE {:.3f}'.format(flow2_EPEs.avg))\n",
        "    # Return Average EPE on Validation Set\n",
        "    return flow2_EPEs.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEng6J_8ewBP"
      },
      "source": [
        "Finally, let's convert the output flow to RGB values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9AfTWhbOcMm"
      },
      "source": [
        "def flow2rgb(flow_map, max_value):\n",
        "    \"\"\"\n",
        "    https://github.com/ClementPinard/FlowNetPytorch/issues/86\n",
        "    \"\"\"\n",
        "    flow_map_np = flow_map.detach().cpu().numpy()\n",
        "    _, h, w = flow_map_np.shape\n",
        "    flow_map_np[:,(flow_map_np[0] == 0) & (flow_map_np[1] == 0)] = float('nan')\n",
        "    rgb_map = np.ones((3,h,w)).astype(np.float32)\n",
        "    if max_value is not None:\n",
        "        normalized_flow_map = flow_map_np / max_value\n",
        "    else:\n",
        "        normalized_flow_map = flow_map_np / (np.abs(flow_map_np).max())\n",
        "    rgb_map[0] += normalized_flow_map[0]\n",
        "    rgb_map[1] -= 0.5*(normalized_flow_map[0] + normalized_flow_map[1])\n",
        "    rgb_map[2] += normalized_flow_map[1]\n",
        "    return rgb_map.clip(0,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHFf6zKTvmsY"
      },
      "source": [
        "Now, let's train it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmLtwOI14Ui0"
      },
      "source": [
        "epochs = 200\n",
        "save_path = '{},{},{}epochs{},b{},lr{}'.format(arch, solver, epochs, ',epochSize'+str(epoch_size) if epoch_size > 0 else '', batch_size, learning_rate)\n",
        "n_iter = 0\n",
        "best_EPE = -1\n",
        "\n",
        "# We'll start from a model pretrained on \"Flying Chairs\" and finetune it to KITTI\n",
        "\n",
        "save_path = os.path.join(\"models\",save_path)\n",
        "\n",
        "print('=> will save everything to {}'.format(save_path))\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "for epoch in range(0, epochs):\n",
        "    scheduler.step()\n",
        "\n",
        "    # Train for one epoch\n",
        "    train_loss, train_EPE = train(train_loader, model, optimizer, epoch, train_writer)\n",
        "    train_writer.add_scalar('mean EPE', train_EPE, epoch)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    with torch.no_grad():\n",
        "        endpointerror = validate(val_loader, model, epoch, output_writers)\n",
        "    test_writer.add_scalar('mean EPE', endpointerror, epoch)\n",
        "\n",
        "    # Store the best EPE\n",
        "    if best_EPE < 0:\n",
        "        best_EPE = endpointerror\n",
        "\n",
        "    is_best = endpointerror < best_EPE\n",
        "    best_EPE = min(endpointerror, best_EPE)\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        'arch': arch,\n",
        "        'state_dict': model.module.state_dict(),\n",
        "        'best_EPE': best_EPE,\n",
        "        'div_flow': div_flow\n",
        "    }, is_best, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz_Z37hHwG1u"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9ayU_G33m63"
      },
      "source": [
        "!ls images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt24naTkpXp_"
      },
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "@torch.no_grad()\n",
        "\n",
        "def inference():\n",
        "    output_string = \"RGB visualization\" # others are \"raw\" or \"both\"\n",
        "    data_dir = \"images\"\n",
        "    save_path = \"output\"\n",
        "\n",
        "    input_transform = transforms.Compose([\n",
        "        flow_transforms.ArrayToTensor(),\n",
        "        transforms.Normalize(mean=[0,0,0], std=[255,255,255]),\n",
        "        transforms.Normalize(mean=[0.411,0.432,0.45], std=[1,1,1])\n",
        "    ])\n",
        "\n",
        "    test_files = sorted(glob.glob(\"images/*.png\"))\n",
        "    img_pairs = list(zip(*[iter(test_files)] * 2))\n",
        "\n",
        "    # create model\n",
        "    network_data = torch.load(\"models/flownetsbn,adam,500epochs,b32,lr0.001/model_best.pth.tar\")\n",
        "    div_flow = network_data['div_flow']\n",
        "    #network_data = torch.load(\"models/flownets_bn_EPE2.459.pth.tar\")\n",
        "    #network_data = torch.load(\"models/flownets_EPE1.951.pth.tar\")\n",
        "\n",
        "    model = flownets(network_data, batchNorm=True).to(device)\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    for (img1_file, img2_file) in tqdm(img_pairs):\n",
        "        print(img1_file)\n",
        "        print(img2_file)\n",
        "        #img1 = input_transform(imread(\"images/000001_10.png\"))\n",
        "        #img2 = input_transform(imread(\"images/000001_11.png\"))\n",
        "        img1 = input_transform(imread(str(img1_file)))\n",
        "        img2 = input_transform(imread(str(img2_file)))     \n",
        "        #cv2_imshow(cv2.cvtColor(cv2.imread(\"dataset/flow_occ/000000_10.png\", -1), cv2.COLOR_YCR_CB2RGB))\n",
        "        input_var = torch.cat([img1, img2]).unsqueeze(0)\n",
        "\n",
        "        input_var = input_var.to(device)\n",
        "        output = model(input_var)\n",
        "        print(len(output))\n",
        "\n",
        "        for suffix, flow_output in zip(['flow', 'inv_flow'], output):\n",
        "            filename = img1_file[:-4]+\"flow\"\n",
        "            rgb_flow = flow2rgb(div_flow * flow_output, max_value=None)\n",
        "            to_save = (rgb_flow * 255).astype(np.uint8).transpose(1,2,0)\n",
        "            cv2_imshow(to_save)\n",
        "            cv2.imwrite(\"output/\"+filename + '.png', to_save)\n",
        "        \n",
        "inference()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2ScRjzLoFdb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}